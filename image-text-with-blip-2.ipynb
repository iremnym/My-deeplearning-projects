{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQF+BgAKN5xLgzS2NpizoL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iremnym/My-deeplearning-projects/blob/main/image-text-with-blip-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio transformers accelerate peft datasets pillow tqdm pandas numpy tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pZRyT2frDgBY",
        "outputId": "d2046ac8-7ffb-4163-c60c-6e892591918f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Found existing installation: transformers 4.51.3\n",
            "Uninstalling transformers-4.51.3:\n",
            "  Successfully uninstalled transformers-4.51.3\n",
            "Found existing installation: accelerate 1.6.0\n",
            "Uninstalling accelerate-1.6.0:\n",
            "  Successfully uninstalled accelerate-1.6.0\n",
            "Found existing installation: peft 0.15.2\n",
            "Uninstalling peft-0.15.2:\n",
            "  Successfully uninstalled peft-0.15.2\n",
            "Found existing installation: datasets 2.14.4\n",
            "Uninstalling datasets-2.14.4:\n",
            "  Successfully uninstalled datasets-2.14.4\n",
            "Found existing installation: pillow 11.2.1\n",
            "Uninstalling pillow-11.2.1:\n",
            "  Successfully uninstalled pillow-11.2.1\n",
            "Found existing installation: tqdm 4.67.1\n",
            "Uninstalling tqdm-4.67.1:\n",
            "  Successfully uninstalled tqdm-4.67.1\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8TwlvG_7P4u",
        "outputId": "8cf73aae-f133-49b0-ec85-ad4123bbf796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet \\\n",
        "  numpy==1.25.2 \\\n",
        "  pandas==2.2.2 \\\n",
        "  torch==2.0.1 \\\n",
        "  torchvision==0.15.2 \\\n",
        "  torchaudio==2.0.2 \\\n",
        "  transformers==4.30.2 \\\n",
        "  accelerate==0.20.3 \\\n",
        "  peft==0.4.0 \\\n",
        "  datasets==2.13.0 \\\n",
        "  pillow==10.0.0 \\\n",
        "  tqdm==4.65.0"
      ],
      "metadata": {
        "id": "7GRiFRBLD0iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "44fe5aca-b28b-4ed9-bf7a-259dd1d3f762"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\n",
            "scikit-image 0.25.2 requires pillow>=10.1, but you have pillow 10.0.0 which is incompatible.\n",
            "dataproc-spark-connect 0.7.3 requires tqdm>=4.67, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments, default_data_collator\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y3j_Rzb_9ZCx",
        "outputId": "450cc1fb-31b8-4fc6-d0a1-1622b27c263d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_id = \"Salesforce/blip-image-captioning-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(model_id)\n",
        "\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_id)\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sgkCAD-U956w",
        "outputId": "e56de926-6c76-48f1-a0ef-d8c0ecac4d72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BlipForConditionalGeneration(\n",
              "  (vision_model): BlipVisionModel(\n",
              "    (embeddings): BlipVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (encoder): BlipEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BlipEncoderLayer(\n",
              "          (self_attn): BlipAttention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): BlipMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (text_decoder): BlipTextLMHeadModel(\n",
              "    (bert): BlipTextModel(\n",
              "      (embeddings): BlipTextEmbeddings(\n",
              "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (encoder): BlipTextEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BlipTextLayer(\n",
              "            (attention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BlipTextIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BlipTextOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BlipTextOnlyMLMHead(\n",
              "      (predictions): BlipTextLMPredictionHead(\n",
              "        (transform): BlipTextPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OBSSDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, processor):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "        self.target_dtype = torch.float16 if torch.cuda.is_available() and model.dtype == torch.float16 else torch.float32\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image_path = os.path.join(self.image_dir, f\"{row['image_id']}.jpg\")\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        caption = row[\"caption\"]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            images=image,\n",
        "            text=caption,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        # Batch dimension'ı sıkıştır\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "        # labels ayarı\n",
        "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
        "        inputs[\"labels\"][inputs[\"attention_mask\"] == 0] = -100\n",
        "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(self.target_dtype)\n",
        "\n",
        "        return inputs\n"
      ],
      "metadata": {
        "id": "jTZSENGe98WH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/obss-competition-files/train.csv\")\n",
        "train_img_dir = (\"/content/drive/MyDrive/obss-competition-files/train/train\")\n",
        "dataset = OBSSDataset(train_df, train_img_dir, processor)\n"
      ],
      "metadata": {
        "id": "uNb4fkoX-AP2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None and \"loss\" in logs:\n",
        "            print(f\"[Trainer] Step: {state.global_step}, Loss: {logs['loss']}\")\n"
      ],
      "metadata": {
        "id": "2fa5RUmV-ZMu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./blip_finetuned\",\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=10,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=4,\n",
        "    dataloader_pin_memory=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    tokenizer=processor,  # uyarı veriyor ama çalışıyor\n",
        "    callbacks=[LoggingCallback()]\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-B8QMAYe-aHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "951b1aeb-f711-47c4-cbb7-836607a794fa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-aba960125b98>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2004' max='2004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2004/2004 23:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.971000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.898500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.798300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.767900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.931400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.177000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.199000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.123700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.127100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.108000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.055100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.084300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.151600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.037400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.010500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.067900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.969700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.068500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.077600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.977900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.969500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.909100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.921600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.945400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.932600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.994400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.907300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.899100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.980500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.015200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.957700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.932000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.893400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.845300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.945700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.881700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.876200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.778600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.829800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.831900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.812400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.824100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.888400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.825500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.877900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>1.874800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.883600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>1.819900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.868700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.799200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.776500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>1.776500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.830400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>1.834200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.817700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>1.742600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>1.831600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>1.772300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>1.768800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.750700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.764600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>1.690500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>1.348900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>1.311700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.297200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>1.292000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>1.329900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.303300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.306700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>1.253000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>1.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.308200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>1.299500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.332100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>1.227900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>1.305100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>1.298100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.243500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.292800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>1.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>1.342400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>1.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>1.288100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.309500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>1.288200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>1.316900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>1.356800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>1.308300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.309000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>1.336100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>1.321400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>1.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>1.294100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.257800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>1.244900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>1.317100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>1.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>1.315300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.236700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>1.251400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>1.302800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>1.246200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>1.283600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.301200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>1.302200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>1.274400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>1.241000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>1.288700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.278300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>1.292900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>1.233700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>1.243500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>1.279500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.263300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>1.226900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>1.224100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>1.332000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>1.303000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.218600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>1.196500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>1.314300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>1.252400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>1.231600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.300300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>1.210400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>1.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>1.247600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>1.120200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.931000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>0.915900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>0.936800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.936000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>0.906900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.925600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>0.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>0.916500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1430</td>\n",
              "      <td>0.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>0.905900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.907200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>0.930300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1470</td>\n",
              "      <td>0.984400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>0.902400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1490</td>\n",
              "      <td>0.882600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.922400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1510</td>\n",
              "      <td>0.965400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>0.915800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1530</td>\n",
              "      <td>0.931000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>0.898500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.941900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>0.906100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1570</td>\n",
              "      <td>0.881300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>0.914900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1590</td>\n",
              "      <td>0.878200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.880900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>0.887200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1630</td>\n",
              "      <td>0.892400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>0.922000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.898200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>0.875100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1670</td>\n",
              "      <td>0.869900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>0.925600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1690</td>\n",
              "      <td>0.855400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1710</td>\n",
              "      <td>0.881900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>0.898800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1730</td>\n",
              "      <td>0.888900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>0.878800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.894100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>0.878300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1770</td>\n",
              "      <td>0.847700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>0.869900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1790</td>\n",
              "      <td>0.847200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.888800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1810</td>\n",
              "      <td>0.853700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1830</td>\n",
              "      <td>0.901700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>0.917200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.942700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>0.856100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1870</td>\n",
              "      <td>0.876600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>0.867800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1890</td>\n",
              "      <td>0.853100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.901900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1910</td>\n",
              "      <td>0.903200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>0.867700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1930</td>\n",
              "      <td>0.865900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>0.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.856000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>0.879300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1970</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990</td>\n",
              "      <td>0.884700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.888500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trainer] Step: 10, Loss: 1.971\n",
            "[Trainer] Step: 20, Loss: 1.8985\n",
            "[Trainer] Step: 30, Loss: 1.7983\n",
            "[Trainer] Step: 40, Loss: 1.7679\n",
            "[Trainer] Step: 50, Loss: 1.7603\n",
            "[Trainer] Step: 60, Loss: 1.9314\n",
            "[Trainer] Step: 70, Loss: 2.177\n",
            "[Trainer] Step: 80, Loss: 2.199\n",
            "[Trainer] Step: 90, Loss: 2.1237\n",
            "[Trainer] Step: 100, Loss: 2.1271\n",
            "[Trainer] Step: 110, Loss: 2.108\n",
            "[Trainer] Step: 120, Loss: 2.0551\n",
            "[Trainer] Step: 130, Loss: 2.0843\n",
            "[Trainer] Step: 140, Loss: 2.1516\n",
            "[Trainer] Step: 150, Loss: 2.0374\n",
            "[Trainer] Step: 160, Loss: 2.0105\n",
            "[Trainer] Step: 170, Loss: 2.0679\n",
            "[Trainer] Step: 180, Loss: 2.009\n",
            "[Trainer] Step: 190, Loss: 1.9697\n",
            "[Trainer] Step: 200, Loss: 2.0685\n",
            "[Trainer] Step: 210, Loss: 2.0776\n",
            "[Trainer] Step: 220, Loss: 1.9779\n",
            "[Trainer] Step: 230, Loss: 2.001\n",
            "[Trainer] Step: 240, Loss: 1.9695\n",
            "[Trainer] Step: 250, Loss: 1.9091\n",
            "[Trainer] Step: 260, Loss: 2.0379\n",
            "[Trainer] Step: 270, Loss: 1.9216\n",
            "[Trainer] Step: 280, Loss: 1.9454\n",
            "[Trainer] Step: 290, Loss: 1.9326\n",
            "[Trainer] Step: 300, Loss: 1.9944\n",
            "[Trainer] Step: 310, Loss: 1.9073\n",
            "[Trainer] Step: 320, Loss: 1.8654\n",
            "[Trainer] Step: 330, Loss: 1.8991\n",
            "[Trainer] Step: 340, Loss: 1.9805\n",
            "[Trainer] Step: 350, Loss: 2.0152\n",
            "[Trainer] Step: 360, Loss: 1.9577\n",
            "[Trainer] Step: 370, Loss: 1.932\n",
            "[Trainer] Step: 380, Loss: 1.8934\n",
            "[Trainer] Step: 390, Loss: 1.8453\n",
            "[Trainer] Step: 400, Loss: 1.9457\n",
            "[Trainer] Step: 410, Loss: 1.8817\n",
            "[Trainer] Step: 420, Loss: 1.8762\n",
            "[Trainer] Step: 430, Loss: 1.7786\n",
            "[Trainer] Step: 440, Loss: 1.8298\n",
            "[Trainer] Step: 450, Loss: 1.8319\n",
            "[Trainer] Step: 460, Loss: 1.8124\n",
            "[Trainer] Step: 470, Loss: 1.8241\n",
            "[Trainer] Step: 480, Loss: 1.8884\n",
            "[Trainer] Step: 490, Loss: 1.8255\n",
            "[Trainer] Step: 500, Loss: 1.8779\n",
            "[Trainer] Step: 510, Loss: 1.8748\n",
            "[Trainer] Step: 520, Loss: 1.8836\n",
            "[Trainer] Step: 530, Loss: 1.8199\n",
            "[Trainer] Step: 540, Loss: 1.8687\n",
            "[Trainer] Step: 550, Loss: 1.7992\n",
            "[Trainer] Step: 560, Loss: 1.7765\n",
            "[Trainer] Step: 570, Loss: 1.7765\n",
            "[Trainer] Step: 580, Loss: 1.8304\n",
            "[Trainer] Step: 590, Loss: 1.8342\n",
            "[Trainer] Step: 600, Loss: 1.8177\n",
            "[Trainer] Step: 610, Loss: 1.7426\n",
            "[Trainer] Step: 620, Loss: 1.8316\n",
            "[Trainer] Step: 630, Loss: 1.7723\n",
            "[Trainer] Step: 640, Loss: 1.7688\n",
            "[Trainer] Step: 650, Loss: 1.7507\n",
            "[Trainer] Step: 660, Loss: 1.7646\n",
            "[Trainer] Step: 670, Loss: 1.6905\n",
            "[Trainer] Step: 680, Loss: 1.3489\n",
            "[Trainer] Step: 690, Loss: 1.3117\n",
            "[Trainer] Step: 700, Loss: 1.2972\n",
            "[Trainer] Step: 710, Loss: 1.292\n",
            "[Trainer] Step: 720, Loss: 1.3008\n",
            "[Trainer] Step: 730, Loss: 1.3299\n",
            "[Trainer] Step: 740, Loss: 1.3033\n",
            "[Trainer] Step: 750, Loss: 1.3067\n",
            "[Trainer] Step: 760, Loss: 1.253\n",
            "[Trainer] Step: 770, Loss: 1.2905\n",
            "[Trainer] Step: 780, Loss: 1.3082\n",
            "[Trainer] Step: 790, Loss: 1.2995\n",
            "[Trainer] Step: 800, Loss: 1.3321\n",
            "[Trainer] Step: 810, Loss: 1.2279\n",
            "[Trainer] Step: 820, Loss: 1.3051\n",
            "[Trainer] Step: 830, Loss: 1.2981\n",
            "[Trainer] Step: 840, Loss: 1.2435\n",
            "[Trainer] Step: 850, Loss: 1.2928\n",
            "[Trainer] Step: 860, Loss: 1.3236\n",
            "[Trainer] Step: 870, Loss: 1.3424\n",
            "[Trainer] Step: 880, Loss: 1.2905\n",
            "[Trainer] Step: 890, Loss: 1.2881\n",
            "[Trainer] Step: 900, Loss: 1.3095\n",
            "[Trainer] Step: 910, Loss: 1.2882\n",
            "[Trainer] Step: 920, Loss: 1.3169\n",
            "[Trainer] Step: 930, Loss: 1.3568\n",
            "[Trainer] Step: 940, Loss: 1.3083\n",
            "[Trainer] Step: 950, Loss: 1.309\n",
            "[Trainer] Step: 960, Loss: 1.3361\n",
            "[Trainer] Step: 970, Loss: 1.3214\n",
            "[Trainer] Step: 980, Loss: 1.2588\n",
            "[Trainer] Step: 990, Loss: 1.2941\n",
            "[Trainer] Step: 1000, Loss: 1.2578\n",
            "[Trainer] Step: 1010, Loss: 1.2449\n",
            "[Trainer] Step: 1020, Loss: 1.3171\n",
            "[Trainer] Step: 1030, Loss: 1.2822\n",
            "[Trainer] Step: 1040, Loss: 1.3153\n",
            "[Trainer] Step: 1050, Loss: 1.2367\n",
            "[Trainer] Step: 1060, Loss: 1.2514\n",
            "[Trainer] Step: 1070, Loss: 1.3028\n",
            "[Trainer] Step: 1080, Loss: 1.2462\n",
            "[Trainer] Step: 1090, Loss: 1.2836\n",
            "[Trainer] Step: 1100, Loss: 1.3012\n",
            "[Trainer] Step: 1110, Loss: 1.3022\n",
            "[Trainer] Step: 1120, Loss: 1.2744\n",
            "[Trainer] Step: 1130, Loss: 1.241\n",
            "[Trainer] Step: 1140, Loss: 1.2887\n",
            "[Trainer] Step: 1150, Loss: 1.2783\n",
            "[Trainer] Step: 1160, Loss: 1.2929\n",
            "[Trainer] Step: 1170, Loss: 1.2337\n",
            "[Trainer] Step: 1180, Loss: 1.2435\n",
            "[Trainer] Step: 1190, Loss: 1.2795\n",
            "[Trainer] Step: 1200, Loss: 1.2633\n",
            "[Trainer] Step: 1210, Loss: 1.2269\n",
            "[Trainer] Step: 1220, Loss: 1.2241\n",
            "[Trainer] Step: 1230, Loss: 1.332\n",
            "[Trainer] Step: 1240, Loss: 1.303\n",
            "[Trainer] Step: 1250, Loss: 1.2186\n",
            "[Trainer] Step: 1260, Loss: 1.1965\n",
            "[Trainer] Step: 1270, Loss: 1.3143\n",
            "[Trainer] Step: 1280, Loss: 1.2524\n",
            "[Trainer] Step: 1290, Loss: 1.2316\n",
            "[Trainer] Step: 1300, Loss: 1.3003\n",
            "[Trainer] Step: 1310, Loss: 1.2104\n",
            "[Trainer] Step: 1320, Loss: 1.297\n",
            "[Trainer] Step: 1330, Loss: 1.2476\n",
            "[Trainer] Step: 1340, Loss: 1.1202\n",
            "[Trainer] Step: 1350, Loss: 0.931\n",
            "[Trainer] Step: 1360, Loss: 0.9159\n",
            "[Trainer] Step: 1370, Loss: 0.9368\n",
            "[Trainer] Step: 1380, Loss: 0.936\n",
            "[Trainer] Step: 1390, Loss: 0.9069\n",
            "[Trainer] Step: 1400, Loss: 0.9256\n",
            "[Trainer] Step: 1410, Loss: 0.9207\n",
            "[Trainer] Step: 1420, Loss: 0.9165\n",
            "[Trainer] Step: 1430, Loss: 0.8925\n",
            "[Trainer] Step: 1440, Loss: 0.9059\n",
            "[Trainer] Step: 1450, Loss: 0.9072\n",
            "[Trainer] Step: 1460, Loss: 0.9303\n",
            "[Trainer] Step: 1470, Loss: 0.9844\n",
            "[Trainer] Step: 1480, Loss: 0.9024\n",
            "[Trainer] Step: 1490, Loss: 0.8826\n",
            "[Trainer] Step: 1500, Loss: 0.9224\n",
            "[Trainer] Step: 1510, Loss: 0.9654\n",
            "[Trainer] Step: 1520, Loss: 0.9158\n",
            "[Trainer] Step: 1530, Loss: 0.931\n",
            "[Trainer] Step: 1540, Loss: 0.8985\n",
            "[Trainer] Step: 1550, Loss: 0.9419\n",
            "[Trainer] Step: 1560, Loss: 0.9061\n",
            "[Trainer] Step: 1570, Loss: 0.8813\n",
            "[Trainer] Step: 1580, Loss: 0.9149\n",
            "[Trainer] Step: 1590, Loss: 0.8782\n",
            "[Trainer] Step: 1600, Loss: 0.8863\n",
            "[Trainer] Step: 1610, Loss: 0.8809\n",
            "[Trainer] Step: 1620, Loss: 0.8872\n",
            "[Trainer] Step: 1630, Loss: 0.8924\n",
            "[Trainer] Step: 1640, Loss: 0.922\n",
            "[Trainer] Step: 1650, Loss: 0.8982\n",
            "[Trainer] Step: 1660, Loss: 0.8751\n",
            "[Trainer] Step: 1670, Loss: 0.8699\n",
            "[Trainer] Step: 1680, Loss: 0.9256\n",
            "[Trainer] Step: 1690, Loss: 0.8554\n",
            "[Trainer] Step: 1700, Loss: 0.888\n",
            "[Trainer] Step: 1710, Loss: 0.8819\n",
            "[Trainer] Step: 1720, Loss: 0.8988\n",
            "[Trainer] Step: 1730, Loss: 0.8889\n",
            "[Trainer] Step: 1740, Loss: 0.8788\n",
            "[Trainer] Step: 1750, Loss: 0.8941\n",
            "[Trainer] Step: 1760, Loss: 0.8783\n",
            "[Trainer] Step: 1770, Loss: 0.8477\n",
            "[Trainer] Step: 1780, Loss: 0.8699\n",
            "[Trainer] Step: 1790, Loss: 0.8472\n",
            "[Trainer] Step: 1800, Loss: 0.8888\n",
            "[Trainer] Step: 1810, Loss: 0.8537\n",
            "[Trainer] Step: 1820, Loss: 0.888\n",
            "[Trainer] Step: 1830, Loss: 0.9017\n",
            "[Trainer] Step: 1840, Loss: 0.9172\n",
            "[Trainer] Step: 1850, Loss: 0.9427\n",
            "[Trainer] Step: 1860, Loss: 0.8561\n",
            "[Trainer] Step: 1870, Loss: 0.8766\n",
            "[Trainer] Step: 1880, Loss: 0.8678\n",
            "[Trainer] Step: 1890, Loss: 0.8531\n",
            "[Trainer] Step: 1900, Loss: 0.9019\n",
            "[Trainer] Step: 1910, Loss: 0.9032\n",
            "[Trainer] Step: 1920, Loss: 0.8677\n",
            "[Trainer] Step: 1930, Loss: 0.8659\n",
            "[Trainer] Step: 1940, Loss: 0.864\n",
            "[Trainer] Step: 1950, Loss: 0.856\n",
            "[Trainer] Step: 1960, Loss: 0.8793\n",
            "[Trainer] Step: 1970, Loss: 0.9\n",
            "[Trainer] Step: 1980, Loss: 0.8875\n",
            "[Trainer] Step: 1990, Loss: 0.8847\n",
            "[Trainer] Step: 2000, Loss: 0.8885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2004, training_loss=1.3660268252957128, metrics={'train_runtime': 1408.5247, 'train_samples_per_second': 45.509, 'train_steps_per_second': 1.423, 'total_flos': 3.803904953127626e+19, 'train_loss': 1.3660268252957128, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.save_model(\"finetuned_blip\")\n",
        "processor.save_pretrained(\"finetuned_blip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IehBiMdbOnKO",
        "outputId": "5a3a87c4-eb30-4ad4-dd36-1e3d9e05bc4a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r finetuned_blip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "-6KI6EYCmfdQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r finetuned_blip.zip finetuned_blip\n",
        "from google.colab import files\n",
        "files.download(\"finetuned_blip.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "-oZMVeQImlZC",
        "outputId": "4b8f003f-be8b-4ad0-8f8a-e07e44fcb598"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: finetuned_blip/ (stored 0%)\n",
            "  adding: finetuned_blip/model.safetensors (deflated 7%)\n",
            "  adding: finetuned_blip/preprocessor_config.json (deflated 48%)\n",
            "  adding: finetuned_blip/generation_config.json (deflated 28%)\n",
            "  adding: finetuned_blip/vocab.txt (deflated 53%)\n",
            "  adding: finetuned_blip/tokenizer_config.json (deflated 74%)\n",
            "  adding: finetuned_blip/config.json (deflated 54%)\n",
            "  adding: finetuned_blip/tokenizer.json (deflated 71%)\n",
            "  adding: finetuned_blip/special_tokens_map.json (deflated 80%)\n",
            "  adding: finetuned_blip/training_args.bin (deflated 52%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85aabb3c-b4ab-4a4c-9924-8d9b3e18ee75\", \"finetuned_blip.zip\", 916959740)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "model_path = \"./finetuned_blip\"\n",
        "processor = BlipProcessor.from_pretrained(model_path)\n",
        "model = BlipForConditionalGeneration.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/obss-competition-files/test.csv\")\n",
        "test_img_dir = \"/content/drive/MyDrive/obss-competition-files/test/test\"\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "prompt = (\n",
        "    \"Describe the image with detailed, vivid, and specific language. \"\n",
        "    \"Focus on the actions, scene context, objects, colors, and interactions. \"\n",
        "    \"Avoid generic captions. Write like a human observer.\"\n",
        ")\n",
        "\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    image_path = os.path.join(test_img_dir, f\"{row['image_id']}.jpg\")\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=60,\n",
        "        num_beams=7,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=1.2,\n",
        "        length_penalty=1.1,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    caption = processor.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "    results.append({\"image_id\": row[\"image_id\"], \"caption\": caption})\n",
        "\n",
        "# Caption sonuçlarını kaydet\n",
        "submission_df = pd.DataFrame(results)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TbzNtdbmvFY",
        "outputId": "55f0dd73-de31-482d-eb14-c0feef7b9a0a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3771 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 3771/3771 [16:04<00:00,  3.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MW1Xl8zQuCvf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}